{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ae563b6-71e2-4956-b73f-45d043fb1bd2",
   "metadata": {},
   "source": [
    "# Leveraging data analytics and machine learning to improve customer satisfaction\n",
    "--- \n",
    "\n",
    "Jose Oliveira da Cruz | jose [at] jfocruz [dot] com\n",
    "\n",
    "\n",
    "## Index of Jupyter Notebook: `nb03_modeling-part2.ipynb`\n",
    "---\n",
    "- [Background](#background)\n",
    "- [Load and clean data](#load)\n",
    "- [Topic Modeling](#model)\n",
    "\n",
    "\n",
    "## Background\n",
    "\n",
    "The available data contains comments from users that can be explored to extract the reasons for a specific score. \n",
    "\n",
    "- What are three main complaints in case of tickets with bad CSAT based on comments?\n",
    "\n",
    "To solve this complex problem, I used NLP Topic Modeling with [Latent Dirichlet Allocation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2ea8b2b-129d-4c35-bde8-99d08c05f92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import os\n",
    "import datetime\n",
    "import missingno as msno # to visualize missing data\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# fix random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "save_figs = True\n",
    "plt.style.use('ggplot')\n",
    "fig_kwargs = dict(bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679dde1f-80ba-49ae-b2cd-9441164e130c",
   "metadata": {},
   "source": [
    "<a id=\"load\"><a/>\n",
    "## Load and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72a4c9be-92de-4911-b3d4-85e8bbbf9108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tickets for which we have CSAT reviews\n",
    "df_tickets_with_satisfaction = pd.read_csv('../data/processed/merged_datasets.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "836185a6-9fff-4129-a3ca-3aae742d26a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract comments\n",
    "comments = df_tickets_with_satisfaction[df_tickets_with_satisfaction.satisfaction.isin(['Bad'])].comment.to_frame().dropna()\n",
    "\n",
    "# Remove comments without information\n",
    "comments = comments[~comments.comment.isin(['N / A'])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f23ad82-aee8-47d1-a943-e8f022e25fb1",
   "metadata": {},
   "source": [
    "<a id=\"model\"><a/>\n",
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8061a4c0-48d0-4d70-99b4-e4ad9901b601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for npl\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords', quiet=True);\n",
    "nltk.download('wordnet', quiet=True);\n",
    "nltk.download('punkt', quiet=True);\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True);\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "STOP_WORDS_ENG = stopwords.words('english')\n",
    "STOP_WORDS_ENG.extend(['la', 'de', 'le', 'pa', 'del', 'el'])\n",
    "ENGLISH_VOCABULARY = list(w.lower() for w in nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95fe7c20-8e44-4250-8e3a-ab0fb2d82851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Case normalize, clean, tokenize, verify english corpus and lemmatize text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tokens_lem : list\n",
    "        List of clean, normalized and lemmatized tokens.\n",
    "    \"\"\"\n",
    "    # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'[^0-9a-zA-Z]', ' ', text)\n",
    "\n",
    "    # tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # remove words that are not in english and with len < 2 char\n",
    "    tokens = [word for word in tokens if word in ENGLISH_VOCABULARY and len(word) > 1]\n",
    "\n",
    "    # lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens_lem = [lemmatizer.lemmatize(token.strip().lower()) for token in tokens\n",
    "                  if token not in STOP_WORDS_ENG]\n",
    "\n",
    "    return tokens_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fee1b1c8-5467-4355-9748-01ac6bb2e6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84c9373e-02c6-4566-942d-4dde2d1a461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many topics?\n",
    "number_of_topics = 3\n",
    "\n",
    "# Create model object\n",
    "model = make_pipeline(TfidfVectorizer(tokenizer=tokenize), # use custom tokenizer\n",
    "                      LatentDirichletAllocation(n_components=number_of_topics,\n",
    "                                                n_jobs=-1,\n",
    "                                                random_state=seed)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cad774a-b375-41f2-938b-3c28cfe8913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model and tranform data\n",
    "id_topic = model.fit_transform(comments.comment)\n",
    "\n",
    "# get feature names (== tokens)\n",
    "vocabulary = model.named_steps.get('tfidfvectorizer').get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddb9406b-72d2-4f25-aabc-f8134d212213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(number_of_top_words, model_pipeline, vocab):\n",
    "    \"\"\"Returns words per topic.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    number_of_top_words : int\n",
    "        Number of words per topic\n",
    "        \n",
    "    model_pipeline : sklearn.pipeline\n",
    "        Must have a fited LDA model\n",
    "    \n",
    "    Notes\n",
    "    ----- \n",
    "    Visualization taken from: https://stackoverflow.com/questions/44208501/getting-topic-word-distribution-from-lda-in-scikit-learn\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    topic_words = {}\n",
    "\n",
    "    for topic, component in enumerate(model.named_steps.get('latentdirichletallocation').components_):\n",
    "        # for the n-dimensional array \"arr\":\n",
    "        # argsort() returns a ranked n-dimensional array of arr, call it \"ranked_array\"\n",
    "        # which contains the indices that would sort arr in a descending fashion\n",
    "        # for the ith element in ranked_array, ranked_array[i] represents the index of the\n",
    "        # element in arr that should be at the ith index in ranked_array\n",
    "        # ex. arr = [3,7,1,0,3,6]\n",
    "        # np.argsort(arr) -> [3, 2, 0, 4, 5, 1]\n",
    "        # word_idx contains the indices in \"topic\" of the top num_top_words most relevant\n",
    "        # to a given topic ... it is sorted ascending to begin with and then reversed (desc. now)    \n",
    "        word_idx = np.argsort(component)[::-1][:number_of_top_words]\n",
    "\n",
    "        # store the words most relevant to the topic\n",
    "        topic_words[topic] = [vocab[i] for i in word_idx]\n",
    "        \n",
    "    for topic, words in topic_words.items():\n",
    "        \n",
    "        print(f'Topic: {topic + 1}')\n",
    "        \n",
    "        print(f'Words:  {\", \".join(words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10aa3af3-2646-4064-981d-cf531bbf2b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 1\n",
      "Words:  service, delivery, mode, inefficient, bad\n",
      "Topic: 2\n",
      "Words:  agent, solution, communication, behavior, slow\n",
      "Topic: 3\n",
      "Words:  like, unreliable, resolution, useless, dont\n"
     ]
    }
   ],
   "source": [
    "display_topics(number_of_top_words=5, model_pipeline=model, vocab=vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f5b0b0-2bc7-49bd-bcbf-4c9eeaa68c58",
   "metadata": {},
   "source": [
    "---\n",
    "2022 - Jose Oliveira da Cruz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
